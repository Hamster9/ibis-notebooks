{
 "metadata": {
  "name": "",
  "signature": "sha256:9a45b538e488e1d59b474238bc6453667e2ae4a5ea6e8d9f1e496a560861131a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import ibis\n",
      "\n",
      "ic = ibis.impala_connect(host='localhost', database='ibis_testing')\n",
      "hdfs = ibis.hdfs_connect(port=5070)\n",
      "con = ibis.make_client(ic, hdfs_client=hdfs)\n",
      "\n",
      "ibis.options.interactive = True"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Creating new Impala tables from Ibis expressions\n",
      "---\n",
      "\n",
      "Suppose you have an Ibis expression that produces a table:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "table = con.table('functional_alltypes')\n",
      "\n",
      "t2 = table[table, (table.bigint_col - table.int_col).name('foo')]\n",
      "\n",
      "expr = (t2\n",
      "        [t2.bigint_col > 30]\n",
      "        .group_by('string_col')\n",
      "        .aggregate([t2.foo.min().name('min_foo'),\n",
      "                    t2.foo.max().name('max_foo'),\n",
      "                    t2.foo.sum().name('sum_foo')]))\n",
      "expr"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "  string_col  min_foo  max_foo  sum_foo\n",
        "0          6       54       54    39420\n",
        "1          8       72       72    52560\n",
        "2          4       36       36    26280\n",
        "3          5       45       45    32850\n",
        "4          7       63       63    45990\n",
        "5          9       81       81    59130"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To create a table in the database from the results of this expression, use the connection's `create_table` method:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "con.create_table('testing_table', expr, database='ibis_testing')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By default, this creates a table stored as **Parquet format** in HDFS. Support for views, external tables, configurable file formats, and so forth, will come in the future. Feedback on what kind of interface would be useful for that would help."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "con.table('testing_table')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "  string_col  min_foo  max_foo  sum_foo\n",
        "0          8       72       72    52560\n",
        "1          5       45       45    32850\n",
        "2          6       54       54    39420\n",
        "3          4       36       36    26280\n",
        "4          7       63       63    45990\n",
        "5          9       81       81    59130"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Tables can be similarly dropped with `drop_table`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "con.drop_table('testing_table', database='ibis_testing')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Inserting data into existing Impala tables\n",
      "---\n",
      "\n",
      "The client's `insert` method can append new data to an existing table or overwrite the data that is in there."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "con.create_table('testing_table', expr)\n",
      "con.table('testing_table')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "  string_col  min_foo  max_foo  sum_foo\n",
        "0          9       81       81    59130\n",
        "1          6       54       54    39420\n",
        "2          4       36       36    26280\n",
        "3          7       63       63    45990\n",
        "4          8       72       72    52560\n",
        "5          5       45       45    32850"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "con.insert('testing_table', expr)\n",
      "con.table('testing_table')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "   string_col  min_foo  max_foo  sum_foo\n",
        "0           8       72       72    52560\n",
        "1           5       45       45    32850\n",
        "2           9       81       81    59130\n",
        "3           6       54       54    39420\n",
        "4           4       36       36    26280\n",
        "5           7       63       63    45990\n",
        "6           6       54       54    39420\n",
        "7           4       36       36    26280\n",
        "8           7       63       63    45990\n",
        "9           9       81       81    59130\n",
        "10          8       72       72    52560\n",
        "11          5       45       45    32850"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "con.drop_table('testing_table')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Uploading / downloading data from HDFS\n",
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you've set up an HDFS connection, you can use the Ibis HDFS interface to look through your data and read and write files to and from HDFS:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hdfs = con.hdfs\n",
      "hdfs.ls('/__ibis/ibis-testing-data')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "[u'/__ibis/ibis-testing-data/avro',\n",
        " u'/__ibis/ibis-testing-data/csv',\n",
        " u'/__ibis/ibis-testing-data/parquet']"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hdfs.ls('/__ibis/ibis-testing-data/parquet')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "[u'/__ibis/ibis-testing-data/parquet/functional_alltypes',\n",
        " u'/__ibis/ibis-testing-data/parquet/tpch_ctas_cancel',\n",
        " u'/__ibis/ibis-testing-data/parquet/tpch_customer',\n",
        " u'/__ibis/ibis-testing-data/parquet/tpch_lineitem',\n",
        " u'/__ibis/ibis-testing-data/parquet/tpch_nation',\n",
        " u'/__ibis/ibis-testing-data/parquet/tpch_orders',\n",
        " u'/__ibis/ibis-testing-data/parquet/tpch_part',\n",
        " u'/__ibis/ibis-testing-data/parquet/tpch_partsupp',\n",
        " u'/__ibis/ibis-testing-data/parquet/tpch_region',\n",
        " u'/__ibis/ibis-testing-data/parquet/tpch_supplier']"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Suppose we wanted to download `/__ibis/ibis-testing-data/parquet/functional_alltypes`, which is a directory. We need only do:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hdfs.get('/__ibis/ibis-testing-data/parquet/functional_alltypes', 'parquet_dir')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "'parquet_dir'"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we have that directory locally:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!ls "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1-Intro-and-Setup.ipynb\r\n",
        "2-Basics-Aggregate-Filter-Limit.ipynb\r\n",
        "3-Projection-Join-Sort.ipynb\r\n",
        "4-More-Value-Expressions.ipynb\r\n",
        "5-IO-Create-Insert-External-Data.ipynb\r\n",
        "6-Advanced-Topics-TopK-SelfJoins.ipynb\r\n",
        "7-Advanced-Topics-ComplexFiltering.ipynb\r\n",
        "8-More-Analytics-Helpers.ipynb\r\n",
        "parquet_dir\r\n",
        "tpch_avro_schemas\r\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Files and directories can be written to HDFS just as easily using `put`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hdfs.put('/__ibis/dir-write-example', 'parquet_dir')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hdfs.ls('/__ibis')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "[u'/__ibis/dir-write-example',\n",
        " u'/__ibis/ibis-testing',\n",
        " u'/__ibis/ibis-testing-data']"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Delete files with `rm` or directories with `rmdir`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hdfs.rmdir('/__ibis/dir-write-example')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!rm -rf parquet_dir/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Queries on Parquet, Avro, and Delimited files in HDFS\n",
      "---\n",
      "\n",
      "Ibis can easily create temporary or persistent Impala tables that reference data in the following formats:\n",
      "\n",
      "- Parquet (`parquet_file`)\n",
      "- Avro (`avro_file`)\n",
      "- Delimited text formats (CSV, TSV, etc.) (`delimited_file`)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Parquet is the easiest because the schema can be read from the data files:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "path = '/__ibis/ibis-testing-data/parquet/tpch_lineitem'\n",
      "\n",
      "lineitem = con.parquet_file(path)\n",
      "lineitem.limit(2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "   l_orderkey  l_partkey  l_suppkey  l_linenumber l_quantity l_extendedprice  \\\n",
        "0           1     155190       7706             1      17.00        21168.23   \n",
        "1           1      67310       7311             2      36.00        45983.16   \n",
        "\n",
        "  l_discount l_tax l_returnflag l_linestatus  l_shipdate l_commitdate  \\\n",
        "0       0.04  0.02            N            O  1996-03-13   1996-02-12   \n",
        "1       0.09  0.06            N            O  1996-04-12   1996-02-28   \n",
        "\n",
        "  l_receiptdate     l_shipinstruct l_shipmode  \\\n",
        "0    1996-03-22  DELIVER IN PERSON      TRUCK   \n",
        "1    1996-04-20   TAKE BACK RETURN       MAIL   \n",
        "\n",
        "                            l_comment  \n",
        "0             egular courts above the  \n",
        "1  ly final dependencies: slyly bold   "
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lineitem.l_extendedprice.sum()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "Decimal('229577310901.20')"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you want to query a Parquet file and also create a table in Impala that remains after your session, you can pass more information to `parquet_file`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "table = con.parquet_file(path, name='my_parquet_table', \n",
      "                         database='ibis_testing',\n",
      "                         persist=True)\n",
      "table.l_extendedprice.sum()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "Decimal('229577310901.20')"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "con.table('my_parquet_table').l_extendedprice.sum()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "Decimal('229577310901.20')"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "con.drop_table('my_parquet_table')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To query delimited files, you need to write down an Ibis schema. At some point we'd like to build some helper tools that will infer the schema for you, all in good time.\n",
      "\n",
      "There's some CSV files in the test folder, so let's use those:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hdfs.get('/__ibis/ibis-testing-data/csv', 'csv-files')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "'csv-files'"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cat csv-files/0.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "GrFVhyFLxr,-0.27258054813466065,33\r\n",
        "aUjIgYQhno,1.9890544559795427,83\r\n",
        "hV8uxd1pw0,0.31763523777393005,19\r\n",
        "dGynToiK41,1.196633053289239,32\r\n",
        "gtnUgFawOB,0.4321537333817887,4\r\n",
        "eC5IzLWC7l,0.3367731583563448,40\r\n",
        "CVzCnVZVm8,0.10355282147720465,52\r\n",
        "GEI7zFOnLh,-0.042896963221357276,57\r\n",
        "0MPfUfbTbV,-0.7690107131205035,68\r\n",
        "OLMQubAOUp,-0.4912896423852966,80\r\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!rm -rf csv-files/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The schema here is pretty simple (see `ibis.schema` for more):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "schema = ibis.schema([('foo', 'string'),\n",
      "                      ('bar', 'double'),\n",
      "                      ('baz', 'int32')])\n",
      "\n",
      "table = con.delimited_file('/__ibis/ibis-testing-data/csv',\n",
      "                           schema)\n",
      "table.limit(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "          foo       bar  baz\n",
        "0  GrFVhyFLxr -0.272581   33\n",
        "1  aUjIgYQhno  1.989054   83\n",
        "2  hV8uxd1pw0  0.317635   19\n",
        "3  dGynToiK41  1.196633   32\n",
        "4  gtnUgFawOB  0.432154    4\n",
        "5  eC5IzLWC7l  0.336773   40\n",
        "6  CVzCnVZVm8  0.103553   52\n",
        "7  GEI7zFOnLh -0.042897   57\n",
        "8  0MPfUfbTbV -0.769011   68\n",
        "9  OLMQubAOUp -0.491290   80"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "table.bar.summary()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "   count  nulls       min       max        sum      mean  approx_nunique\n",
        "0    100      0 -0.769011  1.989054  28.000246  0.280002              10"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For functions like `parquet_file` and `delimited_file`, an HDFS directory must be passed (we'll add support for S3 and other filesystems later) and the directory must contain files all having the same schema."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you have Avro data, you can query it too if you have the full avro schema:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "avro_schema = {\n",
      "    \"fields\": [\n",
      "        {\"type\": [\"int\", \"null\"], \"name\": \"R_REGIONKEY\"},\n",
      "        {\"type\": [\"string\", \"null\"], \"name\": \"R_NAME\"},\n",
      "        {\"type\": [\"string\", \"null\"], \"name\": \"R_COMMENT\"}],\n",
      "    \"type\": \"record\",\n",
      "    \"name\": \"a\"\n",
      "}\n",
      "\n",
      "table = con.avro_file('/__ibis/ibis-testing-data/avro/tpch.region', avro_schema)\n",
      "table"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "   r_regionkey       r_name                                          r_comment\n",
        "0            0       AFRICA  lar deposits. blithely final packages cajole. ...\n",
        "1            1      AMERICA                    hs use ironic, even requests. s\n",
        "2            2         ASIA                    ges. thinly even pinto beans ca\n",
        "3            3       EUROPE      ly final courts cajole furiously final excuse\n",
        "4            4  MIDDLE EAST  uickly special accounts cajole carefully blith..."
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Other helper functions for interacting with the database\n",
      "---\n",
      "\n",
      "We're adding a growing list of useful utility functions for interacting with an Impala cluster on the client object. The idea is that you should be able to do any database-admin-type work with Ibis and not have to switch over to the Impala SQL shell. Any ways we can make this more pleasant, please let us know.\n",
      "\n",
      "Here's some of the features, which we'll give examples for:\n",
      "\n",
      "- Listing and searching for available databases and tables\n",
      "- Creating and dropping databases\n",
      "- Getting table schemas"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "con.list_databases(like='ibis*')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "['ibis_testing']"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "con.list_tables(database='ibis_testing', like='tpch*')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 29,
       "text": [
        "['tpch_ctas_cancel',\n",
        " 'tpch_customer',\n",
        " 'tpch_lineitem',\n",
        " 'tpch_nation',\n",
        " 'tpch_orders',\n",
        " 'tpch_part',\n",
        " 'tpch_partsupp',\n",
        " 'tpch_region',\n",
        " 'tpch_supplier']"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "schema = con.get_schema('functional_alltypes')\n",
      "schema"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "ibis.Schema {  \n",
        "  id               int32\n",
        "  bool_col         boolean\n",
        "  tinyint_col      int32\n",
        "  smallint_col     int32\n",
        "  int_col          int32\n",
        "  bigint_col       int64\n",
        "  float_col        float\n",
        "  double_col       double\n",
        "  date_string_col  string\n",
        "  string_col       string\n",
        "  timestamp_col    timestamp\n",
        "  year             int32\n",
        "  month            int32\n",
        "}"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Databases can be created, too, and you can set the storage path in HDFS you want for the data files"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "db = 'ibis_testing2'\n",
      "con.create_database(db, path='/__ibis/my-test-database')\n",
      "con.create_table('example_table', con.table('functional_alltypes'),\n",
      "                 database=db)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hopefully, there will be data files in the indicated spot in HDFS:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hdfs.ls('/__ibis/my-test-database')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "[u'/__ibis/my-test-database/example_table']"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To drop a database, including all tables in it, you can use `drop_database` with `force=True`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "con.drop_database(db, force=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Dealing with Partitioned tables in Impala\n",
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Placeholder:** This is not yet implemented. If you have use cases, please let us know."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Faster queries on small data in Impala\n",
      "---\n",
      "\n",
      "Since Impala internally uses LLVM to compile parts of queries (aka \"codegen\") to make them faster on large data sets there is a certain amount of overhead with running many kinds of queries, even on small datasets. You can disable LLVM code generation when using Ibis, which may significantly speed up queries on smaller datasets:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numpy.random import rand"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "con.disable_codegen()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t = con.table('functional.alltypes')\n",
      "\n",
      "%timeit (t.double_col + rand()).sum().execute()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10 loops, best of 3: 137 ms per loop\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Turn codegen back on\n",
      "con.disable_codegen(False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit (t.double_col + rand()).sum().execute()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 3: 489 ms per loop\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It's important to remember that codegen is a fixed overhead and will significantly speed up queries on big data"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}