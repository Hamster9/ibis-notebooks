{
 "metadata": {
  "name": "",
  "signature": "sha256:48a95915b1d781e112d52dca83bd5fa7450e65168e5bf201fc5ca1870d693165"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from posixpath import join as pjoin\n",
      "import posixpath\n",
      "\n",
      "import os\n",
      "import shutil\n",
      "import sys\n",
      "import ibis\n",
      "import ibis.util as util"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ic = ibis.impala_connect(host='localhost')\n",
      "hdfs = ibis.hdfs_connect(host='localhost', port=5070)\n",
      "con = ibis.make_client(ic, hdfs_client=hdfs)\n",
      "con"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_db = 'ibis_testing'\n",
      "test_db_location = '/__ibis/ibis-testing'\n",
      "test_data_dir = 'ibis-testing-data'\n",
      "test_data_hdfs_loc = '/__ibis/ibis-testing-data'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Populating HDFS and creating test databse\n",
      "---\n",
      "\n",
      "Let's create a test data warehouse in Parquet format for testing and users"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "con.hdfs.put(test_data_hdfs_loc, test_data_dir, verbose=True, overwrite=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if con.exists_database(test_db):\n",
      "    con.drop_database(test_db, drop_tables=True)\n",
      "con.create_database(test_db, path=test_db_location)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Setup the tables based on parquet files"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "parquet_files = con.hdfs.ls(pjoin(test_data_hdfs_loc, 'parquet'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for path in parquet_files:\n",
      "    head, table_name = posixpath.split(path)\n",
      "    print 'Creating {0}'.format(table_name)\n",
      "    con.parquet_file(path, name=table_name, database=test_db, persist=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "con.table('tpch_customer', database='ibis_testing')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create the Ibis test data archive\n",
      "---\n",
      "From scratch -- assumes an Impala dev environment and `/test-warehouse` is available"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if con.exists_database(test_db):\n",
      "    con.drop_database(test_db, drop_tables=True)\n",
      "con.create_database(test_db, path=test_db_location)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "to_scrape = [('tpch', x) for x in con.list_tables(database='tpch')]\n",
      "to_scrape.append(('functional', 'alltypes'))\n",
      "for db, tname in to_scrape:\n",
      "    table = con.table(tname, database=db)\n",
      "    new_name = '{}_{}'.format(db, tname)\n",
      "    print 'Creating {}'.format(new_name)\n",
      "    con.create_table(new_name, table, database=test_db)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if os.path.exists(test_data_dir):\n",
      "    shutil.rmtree(test_data_dir)\n",
      "os.mkdir(test_data_dir)\n",
      "con.hdfs.get(test_db_location, pjoin(test_data_dir, 'parquet'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "avro_path = '/test-warehouse/tpch.region_avro'\n",
      "os.mkdir(os.path.join(test_data_dir, 'avro'))\n",
      "con.hdfs.get(avro_path, pjoin(test_data_dir, 'avro', 'tpch.region'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Generate CSV testing files"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import pandas.util.testing as tm\n",
      "\n",
      "N = 10\n",
      "nfiles = 10\n",
      "\n",
      "csv_base = os.path.join(test_data_dir, 'csv')\n",
      "\n",
      "df = pd.DataFrame({\n",
      "    'foo': [tm.rands(10) for _ in xrange(N)],\n",
      "    'bar': np.random.randn(N),\n",
      "    'baz': np.random.randint(0, 100, size=N)\n",
      "}, columns=['foo', 'bar', 'baz'])\n",
      "\n",
      "for i in xrange(nfiles):\n",
      "    csv_path = os.path.join(csv_base, '{}.csv'.format(i))\n",
      "    print('Writing {}'.format(csv_path))\n",
      "    df.to_csv(csv_path, index=False, header=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}